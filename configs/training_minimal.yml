# Minimal Training Configuration
model:
  model_type: encoder
  name: test_diarization_model
  
  global_config:
    dropout: 0.1
    batch_size: 2
    d_ff: 4
    device: cpu
  
  encoder:
    d_model: 64
    num_layers: 2
    num_heads: 4
    attention_type: softmax
    activation: GELU
    num_classes: 1  # Binary diarization output

global_config:
  corpus_dir: ./data
  output_dir: ./manifests
  
  # Minimal feature config (yesno is 8kHz, 64 mel bins to match d_model)
  feature_type: fbank
  num_mel_bins: 64
  sampling_rate: 8000

datasets:
  - name: yesno

training:
  epochs: 1
  batch_size: 2
  random_seed: 42
  max_steps: 3
  
  loss:
    main: bce_with_logits  # Binary Cross Entropy with Logits for diarization
  
  optimizer:
    type: adam
    lr: 0.001
    
  scheduler:
    type: constant
  
  checkpoint:
    save_dir: ./test_checkpoints/minimal
    interval: 10
  
  validation:
    interval: 5
    batch_size: 2
  
  logging:
    interval: 1
    tensorboard: false
    wandb: false
  
  performance:
    num_workers: 0
    pin_memory: true
  
  eval_knobs:
    label_type: binary
    max_duration: 10.0  # Use duration-based batching for variable-length sequences

