# Test 6: Precomputed Features with MFCC and Bucketing Sampler
# Test precomputed features strategy, MFCC features, and bucketing sampler

model:
  model_type: encoder_decoder
  name: test_ava_precomputed_mfcc
  
  global_config:
    dropout: 0.1
    batch_size: 2
    d_ff: 4
    device: cpu
  
  encoder:
    d_model: 32
    num_layers: 1
    num_heads: 2
    attention_type: softmax
    activation: GELU
  
  decoder:
    d_model: 32
    num_layers: 1
    num_heads: 2
    attention_type: causal_linear
    activation: SWIGLU
    nb_features: 32
    use_cross_attention: true

global_config:
  corpus_dir: ./data
  output_dir: ./manifests
  force_download: false
  random_seed: 42
  
  # MFCC features instead of fbank
  feature_type: mfcc
  num_mel_bins: 32  # Match d_model
  frame_length: 0.025
  frame_shift: 0.01
  sampling_rate: 8000
  dither: 0.0
  snip_edges: false
  preemph_coeff: 0.97
  window_type: povey
  low_freq: 20.0
  high_freq: -400.0
  num_ceps: 32  # Number of MFCC coefficients
  
  # Data loading configuration - PRECOMPUTED FEATURES
  data_loading:
    strategy: precomputed_features   # Use precomputed features!
    input_strategy:
      num_workers: 0
      executor_type: thread
      use_batch_extract: false
      fault_tolerant: false
      return_audio: false
    sampler:
      type: bucketing               # Test bucketing sampler
      num_buckets: 5
      shuffle: true
      drop_last: true
    dataloader:
      num_workers: 2                # Test with multiple workers
      pin_memory: true
      persistent_workers: false
      prefetch_factor: 2
  
  # Storage and computation
  storage_type: lilcom_chunky
  storage_path: ./features/mfcc_32  # Unique cache for MFCC 32 dimensions
  num_jobs: 1  # Use single process for reliable caching
  device: cpu
  progress_bar: true
  cut_window_seconds: 50  # Window long recordings before extraction

datasets:
  - name: ami
    download_params:
      # Microphone type: ihm (individual headset), sdm (single distant), mdm (multiple distant)
      mic: ihm-mix
      url: http://groups.inf.ed.ac.uk/ami
      annotations: null  # Auto-downloaded
    

training:
  epochs: 1
  batch_size: 1
  random_seed: 42
  max_steps: 5  # Quick smoke test
  
  optimizer:
    type: adam
    lr: 0.001
  
  scheduler:
    type: constant
  
  loss:
    main: bce_with_logits  # Binary cross entropy for diarization
    reduction: mean
  
  checkpoint:
    save_dir: ./checkpoints/test_ava_precomputed
    interval: 2  # Save every 2 steps
    save_total_limit: 5
    snapshot_optimizer: true
    snapshot_scheduler: true
    snapshot_features: true
  
  validation:
    interval: 5
    batch_size: 2
    max_steps: 2  # Limit validation to 2 batches for quick testing
  
  logging:
    interval: 1  # Log every step
    tensorboard: false
    wandb: true
    wandb_entity: digital-future
    wandb_project: TS-DIA
    log_metrics: [loss, accuracy, memory, compute_time]
    log_model: true
  
  performance:
    num_workers: 2  # Test with multiple workers
    pin_memory: true
  
  eval_knobs:
    label_type: binary
    min_speaker_dim: 32  # Match decoder nb_features to ensure consistent tensor shapes
    max_duration: 100.0  # Enable bucketing by setting max_duration


