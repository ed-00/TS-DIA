# =============================================================================
# Complete Training Configuration Example
# =============================================================================
# This configuration demonstrates all features of the universal training pipeline
# including optimizer settings, scheduling, callbacks, logging, and more.

# =============================================================================
# Model Configuration (existing structure)
# =============================================================================
model:
  model_type: encoder_decoder
  name: performer_translator
  
  global_config:
    dropout: 0.1
    batch_size: 32
    d_ff: 4
    device: cpu
  
  encoder:
    d_model: 512
    num_layers: 6
    num_heads: 8
    attention_type: linear
    activation: GEGLU
    nb_features: 256
  
  decoder:
    d_model: 512
    num_layers: 6
    num_heads: 8
    attention_type: causal_linear
    activation: SWIGLU
    nb_features: 256

# =============================================================================
# Dataset Configuration (existing structure)
# =============================================================================
global_config:
  corpus_dir: ./data
  output_dir: ./manifests
  force_download: false
  
  # Feature extraction configuration (unified for all datasets)
  feature_type: fbank  # Options: fbank, mfcc, spectrogram
  num_mel_bins: 80  # Number of mel-frequency bins
  frame_length: 0.025  # Frame length in seconds (25ms)
  frame_shift: 0.01  # Frame shift in seconds (10ms)
  sampling_rate: 16000  # Target sampling rate
  dither: 0.0  # Dithering factor
  snip_edges: false  # Whether to snip edges

datasets:
  - name: yesno

# =============================================================================
# Training Configuration (NEW)
# =============================================================================
training:
  # Basic training settings
  epochs: 1
  batch_size: 2
  random_seed: 42
  max_steps: 3  # Set to override epoch-based training
  
  # Optimizer configuration
  optimizer:
    type: adamw
    lr: 0.0002
    weight_decay: 0.01
    betas: [0.9, 0.999]
    epsilon: 0.00000001
    amsgrad: false
  
  # Learning rate scheduler
  scheduler:
    type: cosine  # Options: cosine, linear, exponential, step, plateau, constant
    min_lr: 0.000001
    max_lr: null  # For cyclic schedulers
    warmup_steps: 1000
    decay_steps: 10000
    num_cycles: 1
    gamma: 0.1  # For step/exponential schedulers
  
  # Gradient management
  gradient_clipping: 1.0
  gradient_accumulation_steps: 4
  
  # Mixed precision training
  mixed_precision: true
  amp_loss_scale: null  # null = dynamic scaling
  
  # Performer-specific settings
  feature_redraw_interval: 100  # Redraw projection matrices every N steps
  fixed_projection: false
  
  # Loss configuration
  loss:
    main: cross_entropy
    label_smoothing: 0.1
    reduction: mean
    auxiliary:
      norm_reg: 0.1
      # contrastive: 0.2  # Uncomment for contrastive auxiliary loss
    focal_alpha: null
    focal_gamma: null
  
  # Validation settings
  validation:
    interval: 500  # Validate every N steps
    batch_size: 64
    metric_for_best_model: val_loss
    greater_is_better: false
  
  # Early stopping
  early_stopping:
    patience: 5
    metric: val_loss
    min_delta: 0.001
    mode: min  # 'min' or 'max'
    restore_best_weights: true
  
  # Checkpoint management
  checkpoint:
    save_dir: ./checkpoints/performer_experiment
    interval: 1000  # Save every N steps
    save_total_limit: 5  # Keep only last 5 checkpoints
    resume: null  # Path to resume from
    snapshot_optimizer: true
    snapshot_scheduler: true
    snapshot_features: true
    save_best_only: false
    monitor_metric: val_loss
  
  # Distributed training
  distributed:
    backend: nccl  # nccl, gloo, mpi
    world_size: 1
    local_rank: 0
    sync_gradient_barrier: true
    find_unused_parameters: false
    gradient_as_bucket_view: false
    static_graph: false
  
  # Logging configuration
  logging:
    interval: 50  # Log every N steps
    tensorboard: true
    wandb: false
    wandb_project: ts-dia-training
    wandb_entity: null
    log_metrics: [loss, accuracy, memory, compute_time]
    log_model: true
    log_gradients: false
  
  # Performance optimization
  performance:
    num_workers: 4
    pin_memory: true
    batch_shim: false
    prefetch_factor: 2
    persistent_workers: true
    compile_model: false  # torch.compile (PyTorch 2.0+)
  
  # Callbacks
  callbacks:
    # - pruning
    # - freeze_layers
    # - dynamic_lr
  
  # Profiling
  profiling: false
  
  # Evaluation knobs (for inference/evaluation and diarization settings)
  eval_knobs:
    batch_size: 128
    beam_width: 5
    sliding_window: 512
    # Diarization-specific settings
    label_type: binary  # Options: binary, speaker_id, custom
    max_duration: null  # Maximum duration per batch for dynamic bucketing
  
  # Hyperparameter tuning (optional)
  tuning:
    library: null  # raytune, optuna
    search_space:
      # lr: [1e-5, 1e-4, 5e-4]
      # batch_size: [32, 64]
      # dropout: [0.1, 0.2, 0.3]

