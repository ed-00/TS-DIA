model:
  model_type: encoder_decoder  # Options: encoder, decoder, encoder_decoder
  name: comprehensive_example_model
  
  # ---------------------------------------------------------------------------
  # Global Configuration (shared across encoder and decoder)
  # ---------------------------------------------------------------------------
  global_config:
    dropout: 0.1              # Dropout probability (0.0 - 1.0)
    batch_size: 128            # Batch size for training
    d_ff: 4                   # Feed-forward expansion factor (hidden_dim = d_model * d_ff)
    device: cuda               # Device: cpu, cuda, cuda:0, cuda:1, etc.
  
  # ---------------------------------------------------------------------------
  # Encoder Configuration (for encoder and encoder_decoder models)
  # ---------------------------------------------------------------------------
  encoder:
    # Core architecture parameters
    input_dim: 23             # Input feature dimension (23 MFCC coefficients from mfcc.conf)
    d_model: 2048              # Model dimension / embedding size
    num_layers: 4             # Number of encoder layers
    num_heads: 8              # Number of attention heads (must divide d_model)
    
    # Attention configuration
    attention_type: softmax    # Options: softmax, linear, causal_linear
    nb_features: null          # Number of random features for linear attention (None = auto)
    
    # Activation function
    activation: REGLU         # Options: GELU, RELU, SILU, GEGLU, REGLU, SWIGLU
    
    # Normalization options
    use_rezero: true         # Use ReZero instead of LayerNorm
    use_scalenorm: true      # Use PreScaleNorm instead of LayerNorm
    
    # Linear attention feature management
    # feature_redraw_interval: 1000  # Redraw random features every N steps (None = never)
    # auto_check_redraw: true        # Automatically check when to redraw
    
  # ---------------------------------------------------------------------------
  # Decoder Configuration (for decoder and encoder_decoder models)
  # ---------------------------------------------------------------------------
  decoder:
    # Core architecture parameters
    input_dim: null            # Input feature dimension (null = use encoder output, or specify for decoder-only)
    d_model: 2048              # Model dimension (should match encoder for encoder_decoder)
    num_layers: 4             # Number of decoder layers
    num_heads: 8              # Number of attention heads
    
    # Attention configuration
    attention_type: softmax  # Options: softmax, linear, causal_linear
    # nb_features: 256          # Number of random features for linear attention
    
    # Activation function
    activation: REGLU        # Options: GELU, RELU, SILU, GEGLU, REGLU, SWIGLU
    
    # Cross-attention (for encoder-decoder)
    use_cross_attention: true # true for encoder_decoder, false for decoder-only LM
    
    # Normalization options
    use_rezero: true         # Use ReZero instead of LayerNorm
    use_scalenorm: true      # Use PreScaleNorm instead of LayerNorm
    
    # Linear attention feature management
    # feature_redraw_interval: 1000  # Redraw random features every N steps
    # auto_check_redraw: true        # Automatically check when to redraw
    
    # Classification head (output projection)
    num_classes: 5         # Number of output classes for ego-centric diarization:
                           # 0: ts (target speaker only)
                           # 1: ts_ovl (target speaker overlapping)
                           # 2: others_sgl (one non-target speaker)
                           # 3: others_ovl (multiple non-target speakers)
                           # 4: ns (non-speech)

training:
  # ---------------------------------------------------------------------------
  # Basic Training Settings
  # ---------------------------------------------------------------------------
  epochs: 100                       # Number of training epochs
  batch_size: 128                    # Training batch size
  random_seed: 42                   # Random seed for reproducibility
  max_steps: null                   # Maximum training steps (null = use epochs)
  
  # ---------------------------------------------------------------------------
  # Optimizer Configuration
  # ---------------------------------------------------------------------------
  optimizer:
    type: adamw                     # Options: adam, adamw, sgd, adagrad, rmsprop, adadelta
    lr: 0.0001                      # Learning rate
    weight_decay: 0.01              # Weight decay / L2 regularization
    betas: [0.9, 0.999]             # Beta parameters for Adam-style optimizers
    epsilon: 0.00000001             # Epsilon for numerical stability (1e-8)
    amsgrad: false                  # Use AMSGrad variant for Adam
    momentum: null                  # Momentum for SGD (0.0 - 1.0)
    nesterov: false                 # Use Nesterov momentum for SGD
  
  # ---------------------------------------------------------------------------
  # Learning Rate Scheduler Configuration
  # ---------------------------------------------------------------------------
  scheduler:
    type: cosine                    # Options: cosine, linear, exponential, step, plateau, constant
    min_lr: 0.000001                # Minimum learning rate
    max_lr: null                    # Maximum learning rate (for cyclic schedulers)
    warmup_steps: 1000              # Number of warmup steps
    decay_steps: 10000              # Number of decay steps
    num_cycles: 1                   # Number of cycles (for cosine with restarts)
    step_size: null                 # Step size for StepLR scheduler
    gamma: 0.1                      # Multiplicative factor for decay
    patience: null                  # Patience for ReduceLROnPlateau
    mode: min                       # Mode for ReduceLROnPlateau: min or max
  
  # ---------------------------------------------------------------------------
  # Gradient Management
  # ---------------------------------------------------------------------------
  gradient_clipping: 1.0            # Max gradient norm (null to disable)
  gradient_accumulation_steps: 4    # Number of gradient accumulation steps
  
  # ---------------------------------------------------------------------------
  # Mixed Precision Training
  # ---------------------------------------------------------------------------
  mixed_precision: true             # Enable mixed precision training (FP16/BF16)
  amp_loss_scale: null              # Loss scaling for AMP (null = dynamic scaling)
  
  # ---------------------------------------------------------------------------
  # Performer-Specific Settings
  # ---------------------------------------------------------------------------
  # feature_redraw_interval: 100      # Redraw projection matrices every N steps
  # fixed_projection: false           # Use fixed projection for Performer
  
  # ---------------------------------------------------------------------------
  # Loss Configuration
  # ---------------------------------------------------------------------------
  loss:
    main: cross_entropy             # Options: cross_entropy, mse, mae, bce, bce_with_logits
    label_smoothing: 0.1            # Label smoothing factor (0.0 = no smoothing)
    reduction: mean                 # Options: mean, sum, none
    
    # Auxiliary losses (optional)
    auxiliary:
      norm_reg: 0.1                 # L2 norm regularization weight
      contrastive: 0.2              # Contrastive loss weight
    
    # Focal loss parameters (for imbalanced datasets)
    focal_alpha: null               # Alpha parameter for focal loss
    focal_gamma: null               # Gamma parameter for focal loss
  
  # ---------------------------------------------------------------------------
  # Validation Configuration
  # ---------------------------------------------------------------------------
  validation:
    interval: 500                   # Validate every N steps
    batch_size: 128                  # Validation batch size
    metric_for_best_model: val_loss # Metric to track for best model
    greater_is_better: false        # Whether higher metric is better
  
  # ---------------------------------------------------------------------------
  # Early Stopping Configuration
  # ---------------------------------------------------------------------------
  early_stopping:
    patience: 5                     # Number of epochs to wait for improvement
    metric: val_loss                # Metric to monitor
    min_delta: 0.001                # Minimum change to qualify as improvement
    mode: min                       # Direction: min or max
    restore_best_weights: true      # Restore model to best checkpoint on early stop
  
  # ---------------------------------------------------------------------------
  # Checkpoint Configuration
  # ---------------------------------------------------------------------------
  checkpoint:
    save_dir: ./outputs/checkpoints/softmax_pretraining  # Directory to save checkpoints
    interval: 1000                  # Save checkpoint every N steps
    save_total_limit: 5             # Maximum number of checkpoints to keep
    resume: null                   # Path to checkpoint to resume from
    snapshot_optimizer: true        # Save optimizer state
    snapshot_scheduler: true        # Save scheduler state
    snapshot_features: true         # Save Performer feature matrices
    save_best_only: false           # Only save best checkpoint
    monitor_metric: val_loss        # Metric to monitor for best checkpoint
  
  # ---------------------------------------------------------------------------
  # Distributed Training Configuration
  # ---------------------------------------------------------------------------
  distributed:
    backend: nccl                   # Options: nccl, gloo, mpi
    world_size: 1                   # Total number of processes
    local_rank: 0                   # Local rank of current process
    sync_gradient_barrier: true     # Synchronize gradients across processes
    find_unused_parameters: false   # Find unused parameters in DDP
    gradient_as_bucket_view: false  # Optimize DDP memory usage
    static_graph: false             # Use static graph optimization in DDP
  
  # ---------------------------------------------------------------------------
  # Logging Configuration
  # ---------------------------------------------------------------------------
  logging:
    interval: 50                    # Log every N steps
    tensorboard: false               # Enable TensorBoard logging
    wandb: true                    # Enable Weights & Biases logging
    wandb_project: TS-DIA  # WandB project name
    wandb_entity: digital-future         # WandB entity/team name
    log_model: true                 # Log model architecture
  
  # ---------------------------------------------------------------------------
  # Performance Optimization Configuration
  # ---------------------------------------------------------------------------
  performance:
    num_workers: 32                  # Number of DataLoader worker processes
    pin_memory: true                # Pin memory for faster GPU transfer
    batch_shim: false               # Use batch shim for irregular batch sizes
    prefetch_factor: 2              # Number of batches to prefetch per worker
    persistent_workers: true        # Keep workers alive between epochs
    compile_model: false            # Use torch.compile (PyTorch 2.0+) not implemented yet
  
  # ---------------------------------------------------------------------------
  # Callbacks (Advanced)
  # ---------------------------------------------------------------------------
  callbacks:
    - gradient_clipping             # Gradient clipping callback
    # - pruning                     # Model pruning callback
    # - freeze_layers               # Layer freezing callback
    # - dynamic_lr                  # Dynamic learning rate adjustment
  
  # ---------------------------------------------------------------------------
  # Profiling (for performance analysis)
  # ---------------------------------------------------------------------------
  profiling: false                  # Enable PyTorch profiler
  
  # ---------------------------------------------------------------------------
  # Evaluation Knobs (for inference and diarization)
  # ---------------------------------------------------------------------------
  eval_knobs:
    batch_size: 128                 # Batch size for evaluation/inference
    beam_width: 5                   # Beam width for beam search
    sliding_window: 512             # Sliding window size for long sequences
    
    # Diarization-specific settings
    label_type: ego                 # Options: binary, ego (5-class ego-centric diarization)
    max_duration: null              # Maximum duration per batch (null = no limit)
  
  # ---------------------------------------------------------------------------
  # Hyperparameter Tuning (Advanced, optional)
  # ---------------------------------------------------------------------------
  tuning:
    library: null                   # Options: raytune, optuna, null (not implemented yet)
    search_space:
      # Uncomment to enable hyperparameter search
      # lr: [0.00001, 0.0001, 0.0005]
      # batch_size: [32, 64]
      # dropout: [0.1, 0.2, 0.3]

global_config:
  force_download: false
  random_seed: 42
  corpus_dir: ./outputs/data
  output_dir: ./outputs/manifests
  storage_path: ./outputs/features
  sampling_rate: 8000

  features:
    feature_type: "mfcc"
    sampling_rate: 8000         # --sample-frequency=8000 from mfcc.conf
    frame_length: 0.025         # --frame-length=25 ms from mfcc.conf
    frame_shift: 0.01           # 80 samples @ 8kHz -> 0.01 s
    num_ceps: 23                # --num-ceps=23 from mfcc.conf (higher than default 12)
    use_energy: false
    low_freq: 20.0              # --low-freq=20 from mfcc.conf
    high_freq: 3700.0           # --high-freq=3700 from mfcc.conf (Nyquist for 8kHz)
    snip_edges: false           # --snip-edges=false from mfcc.conf
    preemph_coeff: 0.97
    window_type: "povey"
    dither: 0.0
    round_to_power_of_two: true
    torchaudio_compatible_mel_scale: true
    norm_filters: false
    cepstral_lifter: 22
    num_jobs: 4
    storage_type: "lilcom_chunky"
    mix_eagerly: true
  
  # Data loading configuration
  data_loading:
    strategy: precomputed_features   # precomputed_features | on_the_fly_features

    input_strategy:
      num_workers: 32
      executor_type: thread
      use_batch_extract: true
      fault_tolerant: false
      return_audio: false

    sampler:
      type: simple               # bucketing | dynamic_bucketing | simple
      max_duration: 300.0        # Maximum duration in seconds per batch (null = no limit)
      num_buckets: 10
      shuffle: true
      drop_last: true

    dataloader:
      num_workers: 32             # if input_strategy.num_workers > 0, this is forced to 0
      pin_memory: true
      persistent_workers: false
      prefetch_factor: 2
  
  # Storage and computation
  storage_type: lilcom_chunky
  num_jobs: 32  # Use single process for reliable caching
  device: cuda
  progress_bar: true
  cut_window_seconds: 50  # Window long recordings before extraction

datasets:

  # - name: ava_avd
  #   download_params:
  #     download_annotations: true
  #     download_videos: true

  # - name: libriheavy_mix
  #   download_params:
  #     # Dataset splits to download
  #     # Options: small (~100h), medium (~900h), large (~9000h), dev, test
  #     dataset_parts: small
      
  #     # Number of speakers per mixture
  #     # Options: 1, 2, 3, 4 (can be int or list)
  #     speaker_counts: [1, 2, 3, 4]
      
  #     # HuggingFace cache directory
  #     cache_dir: null
    
  #   process_params:
  #     # Dataset splits to process
  #     dataset_parts: small

  #     # Speaker counts to process
  #     speaker_counts: [1, 2, 3, 4]
      
  #     # Speaker filtering
  #     min_speakers: 1
  #     max_speakers: 4
      
  #     # Custom split mapping (optional)
  #     splits: null

  - name: ami
    download_params:
      # Microphone type: ihm (individual headset), sdm (single distant), mdm (multiple distant)
      mic: ihm-mix
      url: http://groups.inf.ed.ac.uk/ami
      annotations: null  # Auto-downloaded
    
    process_params:
      # Microphone type to process (must match download)
      mic: ihm-mix
      
      # Partition selection
      # Options: full-corpus, full-corpus-asr, scenario-only
      partition: full-corpus
      
      # Text normalization
      # Options: none, upper, kaldi
      normalize_text: none
      
      # Segmentation options
      max_words_per_segment: null  # null = no splitting
      merge_consecutive: false  # Merge same-speaker consecutive segments

  # - name: icsi
  #   download_params:
  #     # Microphone type: ihm (individual headset), mdm (multiple distant)
  #     mic: ihm-mix
  #     url: http://groups.inf.ed.ac.uk/ami
  #     transcripts_dir: ./outputs/data/icsi/ICSI

  #   process_params:
  #     # Microphone type to process
  #     mic: ihm-mix
      
  #     # Text normalization
  #     # Options: none, upper, kaldi
  #     normalize_text: none
      
  #     # Audio format conversion
  #     save_to_wav: true  # Convert SPH files to WAV if needed
  #     output_dir: ./outputs/manifests/icsi
  #     audio_dir: ./outputs/data/icsi/speech
  #     transcripts_dir: ./outputs/data/icsi/ICSI

  # - name: aishell4
  #   download_params:
  #     # Base URL for downloads
  #     base_url: http://www.openslr.org/resources
    
  #   process_params:
  #     # Chinese text normalization
  #     normalize_text: false  # Set true to normalize Chinese characters

  # - name: voxconverse
  #   download_params:
  #     corpus_dir: null  # Will use global_config corpus_dir/voxconverse
    
  #   process_params:
  #     split_test: false  # Whether to split test set


  # - name: ego4d
  #   download_params:
  #     # Dataset parts to download
  #     # Options: clips, annotations, metadata, takes, captures, etc.
  #     dataset_parts: ["clips", "annotations"]
      
  #     # Auto-install Ego4D CLI if not available
  #     install_cli: true
      
  #     # Download timeout (seconds)
  #     timeout: 3600
      
  #     # Path to .env file for environment variables
  #     env_file: ./.env  # Load AWS credentials from project .env file
    
  #   process_params:
  #     # Audio extraction settings
  #     extract_audio: true
  #     audio_sample_rate: 16000
      
  #     # Segment duration constraints
  #     min_segment_duration: 0.5  # seconds
  #     max_segment_duration: 30.0  # seconds
      
  #     # Limit number of clips for testing (0 = no limit)
  #     max_clips: 0
      
  #     # Filter annotations by type (e.g., "av" for audio-visual)
  #     annotation_subset: "av"  # null = all annotations

  # - name: mswild
  #   download_params:
  #     # Multi-modal data downloads
  #     download_audio: true  # ~7.56 GB (required)
  #     download_video: false  # ~43.14 GB (manual download required)
  #     download_faces: false  # ~14.49 GB (manual download required)

    # process_params:
    #   # Split name to RTTM pattern mapping
    #   # Default: {"train": "few_train", "dev": "few_val", "test": "many_val"}
    #   splits:
    #     train: few_train
    #     dev: few_val
    #     test: many_val


  # - name: voxceleb1
  #   download_params: {}
    
  #   # Note: VoxCeleb processing combines voxceleb1 and voxceleb2
  #   # Use voxceleb1_root in process_params
  #   process_params: {}

  # - name: voxceleb2
  #   download_params: {}
    
  #   # Note: VoxCeleb processing combines voxceleb1 and voxceleb2
  #   # Use voxceleb2_root in process_params
  #   process_params: {}

  # - name: tedlium
  #   download_params: {}
    
  #   process_params:
  #     # TED-LIUM root directory
  #     tedlium_root: null
      
  #     # Dataset parts to process
  #     # Options: null (auto-detect), or specific parts
  #     dataset_parts: null
      
  #     # Text normalization
  #     # Options: none, upper, lower, kaldi
  #     normalize_text: none

  # - name: libricss
  #   download_params: {}
    
  #   process_params:
  #     # Recording type
  #     # Options: mdm (multiple distant microphone)
  #     type: ihm-mix
      
  #     # Generate segmented cuts
  #     segmented_cuts: false
      
  # - name: chime6
  #   download_params: {}
    
  #   process_params:
  #     # Dataset parts to process
  #     # Options: all, train, dev, eval
  #     dataset_parts: all
      
  #     # Microphone type: mdm (multiple distant microphone)
  #     mic: ihm-mix
      
  #     # Use reference array
  #     use_reference_array: false
      
  #     # Perform array synchronization
  #     perform_array_sync: false
      
  #     # Verify MD5 checksums
  #     verify_md5_checksums: false
      
  #     # Number of threads per job
  #     num_threads_per_job: 1
      
  #     # Path to sox binary
  #     sox_path: /usr/bin/sox
      
  #     # Text normalization
  #     normalize_text: null
      
  #     # Use CHiME-7 split
  #     use_chime7_split: false