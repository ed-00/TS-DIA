model:
  model_type: encoder_decoder # Options: encoder, decoder, encoder_decoder
  name: softmax_pretraining_model

  # ---------------------------------------------------------------------------
  # Global Configuration (shared across encoder and decoder)
  # ---------------------------------------------------------------------------
  global_config:
    dropout: 0.1 # Dropout probability (0.0 - 1.0)
    batch_size: 256 # Batch size for training (matching EEND reference)
    d_ff: 8 # Feed-forward expansion factor (hidden_dim = d_model * d_ff)
    device: cuda # Device: cpu, cuda, cuda:0, cuda:1, etc.

  # ---------------------------------------------------------------------------
  # Encoder Configuration (for encoder and encoder_decoder models)
  # ---------------------------------------------------------------------------
  encoder:
    # Core architecture parameters
    input_dim:
      345 # Input feature dimension (23 MFCC × 15 context frames from splicing)
      # Features are spliced: 23-dim MFCC with ±7 context frames = 345-dim
      # Splicing: 7 before + 1 current + 7 after = 15 total × 23 = 345
    d_model: 345 # Model dimension / embedding size
    num_layers: 4 # Number of encoder layers
    num_heads: 5 # Number of attention heads (matching EEND: 4 heads)

    # Attention configuration
    attention_type: softmax # Options: softmax, linear, causal_linear
    nb_features: null # Number of random features for linear attention (None = auto)

    # Activation function
    activation: REGLU # Options: GELU, RELU, SILU, GEGLU, REGLU, SWIGLU

    # Normalization options
    use_rezero: true # Use ReZero instead of LayerNorm
    use_scalenorm: true # Use PreScaleNorm instead of LayerNorm

    # Linear attention feature management
    # feature_redraw_interval: 1000  # Redraw random features every N steps (None = never)
    # auto_check_redraw: true        # Automatically check when to redraw

  # ---------------------------------------------------------------------------
  # Decoder Configuration (for decoder and encoder_decoder models)
  # ---------------------------------------------------------------------------

  decoder:
    # Core architecture parameters
    input_dim: 345 # Input feature dimension (null = use encoder output, or specify for decoder-only)
    d_model: 345 # Model dimension (should match encoder for encoder_decoder)
    num_layers: 4 # Number of decoder layers
    num_heads: 5 # Number of attention heads

    # Attention configuration
    attention_type: softmax # Options: softmax, linear, causal_linear
    # nb_features: 256          # Number of random features for linear attention

    # Activation function
    activation: REGLU # Options: GELU, RELU, SILU, GEGLU, REGLU, SWIGLU

    # Cross-attention (for encoder-decoder)
    use_cross_attention: true # true for encoder_decoder, false for decoder-only LM

    # Normalization options
    use_rezero: true # Use ReZero instead of LayerNorm
    use_scalenorm: true # Use PreScaleNorm instead of LayerNorm

    # Linear attention feature management
    # feature_redraw_interval: 1000  # Redraw random features every N steps
    # auto_check_redraw: true        # Automatically check when to redraw

    # Classification head (output projection)
    num_classes:
      5 # Number of output classes for ego-centric diarization:
      # 0: ts (target speaker only)
      # 1: ts_ovl (target speaker overlapping)
      # 2: others_sgl (one non-target speaker)
      # 3: others_ovl (multiple non-target speakers)
      # 4: ns (non-speech)

training:
  # ---------------------------------------------------------------------------
  # Basic Training Settings
  # ---------------------------------------------------------------------------
  epochs: 100 # Number of training epochs
  batch_size: 256 # Training batch size
  random_seed: 42 # Random seed for reproducibility
  max_steps: null # Maximum training steps (null = use epochs)
  training_dataset_map:
    combine: true # Combine training sets if multiple datasets are used
    splits:
      - dataset_name: "simu_1spk"
        split_name: "train_b2_mix100000"
        subset_ratio: 1 # Use 20% of dataset
      - dataset_name: "simu_2spk"
        split_name: "train_b2_mix100000"
        subset_ratio: 1 # Use 20% of dataset
      - dataset_name: "simu_3spk"
        split_name: "train_b5_mix100000"
        subset_ratio: 1 # Use 20% of dataset
      - dataset_name: "simu_4spk"
        split_name: "train_b9_mix100000"
        subset_ratio: 1 # Use 20% of dataset
      - dataset_name: "simu_5spk"
        split_name: "train_b13_mix100000"
        subset_ratio: 1 # Use 20% of dataset

  # ---------------------------------------------------------------------------
  # Optimizer Configuration
  # ---------------------------------------------------------------------------
  optimizer:
    type: adamw # Options: adam, adamw, sgd, adagrad, rmsprop, adadelta
    lr: 0.0001 # Learning rate
    weight_decay: 0.01 # Weight decay / L2 regularization
    betas: [0.9, 0.999] # Beta parameters for Adam-style optimizers
    epsilon: 0.00000001 # Epsilon for numerical stability (1e-8)
    amsgrad: false # Use AMSGrad variant for Adam
    momentum: null # Momentum for SGD (0.0 - 1.0)
    nesterov: false # Use Nesterov momentum for SGD

  # ---------------------------------------------------------------------------
  # Learning Rate Scheduler Configuration
  # ---------------------------------------------------------------------------
  scheduler:
    type: cosine # Options: cosine, linear, exponential, step, plateau, constant
    min_lr: 0.000001 # Minimum learning rate
    max_lr: null # Maximum learning rate (for cyclic schedulers)
    warmup_steps: 25850 # Number of warmup steps
    num_cycles: 1 # Number of cycles (for cosine with restarts)
    step_size: null # Step size for StepLR scheduler
    gamma: 0.1 # Multiplicative factor for decay
    patience: null # Patience for ReduceLROnPlateau
    mode: min # Mode for ReduceLROnPlateau: min or max

  # ---------------------------------------------------------------------------
  # Gradient Management
  # ---------------------------------------------------------------------------
  gradient_clipping: 5.0 # Max gradient norm (matching EEND: 5.0)
  gradient_accumulation_steps: 4 # Number of gradient accumulation steps

  # ---------------------------------------------------------------------------
  # Mixed Precision Training
  # ---------------------------------------------------------------------------
  mixed_precision: true # Enable mixed precision training (FP16/BF16)
  amp_loss_scale: null # Loss scaling for AMP (null = dynamic scaling)

  # ---------------------------------------------------------------------------
  # Performer-Specific Settings
  # ---------------------------------------------------------------------------
  # feature_redraw_interval: 100      # Redraw projection matrices every N steps
  # fixed_projection: false           # Use fixed projection for Performer

  # ---------------------------------------------------------------------------
  # Loss Configuration
  # ---------------------------------------------------------------------------
  loss:
    main: cross_entropy # Options: cross_entropy, mse, mae, bce, bce_with_logits
    reduction: mean # Options: mean, sum, null

    # # Auxiliary losses (optional)
    # auxiliary:
    #   norm_reg: 0.1 # L2 norm regularization weight

    # # Focal loss parameters (for imbalanced datasets)
    # focal_alpha: null # Alpha parameter for focal loss
    # focal_gamma: null # Gamma parameter for focal loss

  # ---------------------------------------------------------------------------
  # Validation Configuration
  # ---------------------------------------------------------------------------
  validation:
    interval: 1 # Validate every N epochs
    batch_size: 256 # Validation batch size (matching training)
    metric_for_best_model: val_loss # Metric to track for best model
    greater_is_better: false # Whether higher metric is better
    validation_dataset_map:
      combine: false # Keep validation splits separate
      splits:
        - dataset_name: "simu_1spk"
          split_name: "dev_b2_mix500"
          subset_ratio: 1.0
        - dataset_name: "simu_2spk"
          split_name: "dev_b2_mix500"
          subset_ratio: 1.0
        - dataset_name: "simu_2spk"
          split_name: "dev_b3_mix500"
          subset_ratio: 1.0
        - dataset_name: "simu_2spk"
          split_name: "dev_b5_mix500"
          subset_ratio: 1.0
        - dataset_name: "simu_3spk"
          split_name: "dev_b5_mix500"
          subset_ratio: 1.0
        - dataset_name: "simu_3spk"
          split_name: "dev_b7_mix500"
          subset_ratio: 1.0
        - dataset_name: "simu_3spk"
          split_name: "dev_b11_mix500"
          subset_ratio: 1.0
        - dataset_name: "simu_4spk"
          split_name: "dev_b9_mix500"
          subset_ratio: 1.0
        - dataset_name: "simu_5spk"
          split_name: "dev_b13_mix500"
          subset_ratio: 1.0
        - dataset_name: "ava_avd"
          split_name: "val"
          subset_ratio: 1.0
        - dataset_name: "ami"
          split_name: "dev"
          subset_ratio: 1.0
        - dataset_name: "icsi"
          split_name: "dev"
          subset_ratio: 1.0
        - dataset_name: "aishell4"
          split_name: "test"
          subset_ratio: 1.0
        - dataset_name: "voxconverse"
          split_name: "dev"
          subset_ratio: 1.0
        - dataset_name: "mswild"
          split_name: "dev"
          subset_ratio: 1.0

  # ---------------------------------------------------------------------------
  # Early Stopping Configuration
  # ---------------------------------------------------------------------------
  early_stopping:
    patience: 100 # Number of epochs to wait for improvement
    metric: val_loss # Metric to monitor
    min_delta: 0.00001 # Minimum change to qualify as improvement
    mode: min # Direction: min or max
    restore_best_weights: true # Restore model to best checkpoint on early stop

  # ---------------------------------------------------------------------------
  # Checkpoint Configuration
  # ---------------------------------------------------------------------------
  checkpoint:
    save_dir: ./outputs/checkpoints/softmax_pretraining_full # Directory to save checkpoints
    interval: 2585 # Save checkpoint every N steps
    save_total_limit: 100 # Maximum number of checkpoints to keep
    resume: null # Path to checkpoint to resume from

  # ---------------------------------------------------------------------------
  # Distributed Training Configuration
  # ---------------------------------------------------------------------------
  # distributed:
  #   backend: nccl # Options: nccl, gloo, mpi
  #   world_size: 1 # Total number of processes
  #   local_rank: 0 # Local rank of current process
  #   sync_gradient_barrier: true # Synchronize gradients across processes
  #   find_unused_parameters: false # Find unused parameters in DDP
  #   gradient_as_bucket_view: false # Optimize DDP memory usage
  #   static_graph: false # Use static graph optimization in DDP

  # ---------------------------------------------------------------------------
  # Logging Configuration
  # ---------------------------------------------------------------------------
  logging:
    interval: 10 # Log every N steps
    tensorboard: false # Enable TensorBoard logging
    wandb: true # Enable Weights & Biases logging
    wandb_project: TS-DIA # WandB project name
    wandb_entity: digital-future # WandB entity/team name
    log_model: true # Log model architecture

  # ---------------------------------------------------------------------------
  # Callbacks (Advanced)
  # ---------------------------------------------------------------------------
  callbacks:
    - gradient_clipping # Gradient clipping callback
    # - pruning                     # Model pruning callback
    # - freeze_layers               # Layer freezing callback
    # - dynamic_lr                  # Dynamic learning rate adjustment

  # ---------------------------------------------------------------------------
  # Profiling (for performance analysis)
  # ---------------------------------------------------------------------------
  profiling: false # Enable PyTorch profiler

  # ---------------------------------------------------------------------------
  # Evaluation Knobs (for inference and diarization)
  # ---------------------------------------------------------------------------
  eval_knobs:
    batch_size: 256 # Batch size for evaluation/inference (matching training)
    beam_width: 5 # Beam width for beam search
    sliding_window: 512 # Sliding window size for long sequences

    # Diarization-specific settings
    label_type: ego # Options: binary, ego (5-class ego-centric diarization)
    max_duration: null # Maximum duration per batch (null = no limit)

  # ---------------------------------------------------------------------------
  # Hyperparameter Tuning (Advanced, optional)
  # ---------------------------------------------------------------------------
  tuning:
    library: null # Options: raytune, optuna, null (not implemented yet)
    search_space:
      # Uncomment to enable hyperparameter search
      # lr: [0.00001, 0.0001, 0.0005]
      # batch_size: [32, 64]
      # dropout: [0.1, 0.2, 0.3]

global_config:
  force_download: false
  random_seed: 42
  corpus_dir: ./outputs/data
  output_dir: ./outputs/manifests
  storage_path: ./outputs/features_storage
  cache_dir: ./outputs/processed_cache/softmax_pretraining_model
  sampling_rate: 8000

  features:
    feature_type: "fbank" # Log Mel filterbank features
    sampling_rate: 8000 # 8kHz sampling rate
    frame_length: 0.025 # 25 ms (200 samples @ 8kHz)
    frame_shift: 0.01 # 10 ms (80 samples @ 8kHz)
    num_mel_bins: 23 # 23-dimensional log Mel features (matching EEND)
    use_energy: false # No energy feature
    low_freq: 0.0 # fmin=0 (EEND uses 0, not 20)
    high_freq: 4000.0 # fmax=4000 (Nyquist for 8kHz)
    snip_edges: false # Keep all frames
    preemph_coeff: 0.97 # Preemphasis coefficient (matching EEND)
    window_type: "hanning" # Hann window (Lhotse uses "hanning" not "hann")
    dither: 0.0 # No dithering
    round_to_power_of_two: true
    remove_dc_offset: true # Standard preprocessing
    raw_energy: true # Use raw energy if needed
    use_fft_mag: false # Use power spectrum
    energy_floor: 1e-10 # Small epsilon for log
    storage_type: "lilcom_chunky"

    vtln_low: 100.0
    vtln_high: -500.0
    debug_mel: False
    htk_mode: False
    use_log_fbank: True
    use_power: True
    htk_compat: False
    num_workers: 4
    # frame option
    blackman_coeff: 0.42
    device: "cuda"
    overwrite: false
    batch_duration: 21000.0

  # Data loading configuration (matching EEND exactly)
  data_loading:
    strategy:
      precomputed_features # not used any more
      # Features are 23-dim log Mel filterbank directly
      # Mean normalization applied per utterance in dataset
    subsampling:
      10 # Subsample by 10x (matching EEND subsampling factor)
      # 30s @ 10ms shift = 3000 frames → 300 frames after subsampling
      # EEND uses 500 frames (5s segments), we use variable length
    chunk_size:
      5.0 # Cut audio into 5-second chunks for uniform training segments
      # This ensures consistent sequence lengths and memory usage

    dataloader:
      num_workers: 8
      pin_memory: true
      persistent_workers: true
      prefetch_factor: 4

datasets:
  - name: simu_1spk
  - name: simu_2spk
  - name: simu_4spk
  - name: simu_3spk
  - name: simu_5spk

  - name: ava_avd
    download_params:
      download_annotations: true
      download_videos: true
  - name: ami
    download_params:
      # Microphone type: ihm (individual headset), sdm (single distant), mdm (multiple distant)
      mic: ihm-mix
      url: http://groups.inf.ed.ac.uk/ami
      annotations: null # Auto-downloaded

    process_params:
      # Microphone type to process (must match download)
      mic: ihm-mix

      # Partition selection
      # Options: full-corpus, full-corpus-asr, scenario-only
      partition: full-corpus

      # Text normalization
      # Options: none, upper, kaldi
      normalize_text: none

      # Segmentation options
      max_words_per_segment: null # null = no splitting
      merge_consecutive: false # Merge same-speaker consecutive segments

  - name: icsi
    download_params:
      # Microphone type: ihm (individual headset), mdm (multiple distant)
      mic: ihm-mix
      url: http://groups.inf.ed.ac.uk/ami
      transcripts_dir: ./outputs/data/icsi/ICSI

    process_params:
      # Microphone type to process
      mic: ihm-mix

      # Text normalization
      # Options: none, upper, kaldi
      normalize_text: none

      # Audio format conversion
      save_to_wav: true # Convert SPH files to WAV if needed
      output_dir: ./outputs/manifests/icsi
      audio_dir: ./outputs/data/icsi/speech
      transcripts_dir: ./outputs/data/icsi/ICSI

  - name: aishell4
    download_params:
      # Base URL for downloads
      base_url: http://www.openslr.org/resources

    process_params:
      # Chinese text normalization
      normalize_text: false # Set true to normalize Chinese characters

  - name: voxconverse
    download_params:
      corpus_dir: null # Will use global_config corpus_dir/voxconverse

    process_params:
      split_test: false # Whether to split test set

  - name: mswild
    download_params:
      # Multi-modal data downloads
      download_audio: true  # ~7.56 GB (required)
      download_video: false  # ~43.14 GB (manual download required)
      download_faces: false  # ~14.49 GB (manual download required)

    process_params:
      # Split name to RTTM pattern mapping
      # Default: {"train": "few_train", "dev": "few_val", "test": "many_val"}
      splits:
        train: few_train
        dev: few_val
        test: many_val


  # - name: ego4d
  #   download_params:
  #     # Dataset parts to download
  #     # Options: clips, annotations, metadata, takes, captures, etc.
  #     dataset_parts: ["clips", "annotations"]

  #     # Auto-install Ego4D CLI if not available
  #     install_cli: true

  #     # Download timeout (seconds)
  #     timeout: 3600

  #     # Path to .env file for environment variables
  #     env_file: ./.env  # Load AWS credentials from project .env file

  #   process_params:
  #     # Audio extraction settings
  #     extract_audio: true
  #     audio_sample_rate: 16000

  #     # Segment duration constraints
  #     min_segment_duration: 0.5  # seconds
  #     max_segment_duration: 30.0  # seconds

  #     # Limit number of clips for testing (0 = no limit)
  #     max_clips: 0

  #     # Filter annotations by type (e.g., "av" for audio-visual)
  #     annotation_subset: "av"  # null = all annotations

  # - name: voxceleb1
  #   download_params: {}

  #   # Note: VoxCeleb processing combines voxceleb1 and voxceleb2
  #   # Use voxceleb1_root in process_params
  #   process_params: {}

  # - name: voxceleb2
  #   download_params: {}

  #   # Note: VoxCeleb processing combines voxceleb1 and voxceleb2
  #   # Use voxceleb2_root in process_params
  #   process_params: {}

  # - name: tedlium
  #   download_params: {}

  #   process_params:
  #     # TED-LIUM root directory
  #     tedlium_root: null

  #     # Dataset parts to process
  #     # Options: null (auto-detect), or specific parts
  #     dataset_parts: null

  #     # Text normalization
  #     # Options: none, upper, lower, kaldi
  #     normalize_text: none

  # - name: libricss
  #   download_params: {}

  #   process_params:
  #     # Recording type
  #     # Options: mdm (multiple distant microphone)
  #     type: ihm-mix

  #     # Generate segmented cuts
  #     segmented_cuts: false

  # - name: chime6
  #   download_params: {}

  #   process_params:
  #     # Dataset parts to process
  #     # Options: all, train, dev, eval
  #     dataset_parts: all

  #     # Microphone type: mdm (multiple distant microphone)
  #     mic: ihm-mix

  #     # Use reference array
  #     use_reference_array: false

  #     # Perform array synchronization
  #     perform_array_sync: false

  #     # Verify MD5 checksums
  #     verify_md5_checksums: false

  #     # Number of threads per job
  #     num_threads_per_job: 1

  #     # Path to sox binary
  #     sox_path: /usr/bin/sox

  #     # Text normalization
  #     normalize_text: null

  #     # Use CHiME-7 split
  #     use_chime7_split: false
