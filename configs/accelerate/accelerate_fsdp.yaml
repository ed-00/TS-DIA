# Accelerate Configuration for FSDP (Fully Sharded Data Parallel)
# Usage: ./docker/run_distributed.sh configs/train/training_example.yml 8 ts-dia-training my-exp configs/accelerate_fsdp.yaml
# Best for large models that don't fit in single GPU memory

compute_environment: LOCAL_MACHINE
debug: false

# Distributed settings
distributed_type: FSDP
downcast_bf16: 'no'

# FSDP specific settings
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch: BACKWARD_PRE
  fsdp_cpu_ram_efficient_loading: true
  fsdp_forward_prefetch: false
  fsdp_offload_params: false
  fsdp_sharding_strategy: FULL_SHARD  # Options: FULL_SHARD, SHARD_GRAD_OP, NO_SHARD, HYBRID_SHARD
  fsdp_state_dict_type: FULL_STATE_DICT
  fsdp_sync_module_states: true
  fsdp_use_orig_params: true

# GPU configuration
gpu_ids: all
machine_rank: 0
main_training_function: main
num_machines: 1
num_processes: 8  # Set to number of GPUs (e.g., 8 for 8 GPUs, 4 for 4 GPUs)

# Mixed precision
mixed_precision: bf16  # Options: no, fp16, bf16

# Other settings
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false

